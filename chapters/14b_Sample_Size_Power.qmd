
# Sample Size and Power



## The Big Picture: Why do we calculate sample size?


In the planning stage, we face a "Goldilocks" problem: 

  * Too Small: You might miss a real difference between groups because your study can't distinguish the signal from the noise (random variation).
  
  * Too Large: You waste money, time, and expose more participants than necessary to an intervention (ethical concerns).
  
  
  
### The "Sample Size Samba" 

Remember that in reality, this is an iterative process (a "samba") between the scientific ideal and the budget/logistical reality. 

You calculate, check the budget, adjust the MDE or power, and calculate again
  
  
### The Goal: We want a sample size just large enough to distinguish a true difference from random variation



## The Four Pillars of Power Calculations

**Definition of Power:** The probability of correctly rejecting the null hypothesis when it is false


### 1. Hypotheses

You must specify what you are testing:

  * Null Hypothesis ($H_0$): There is no difference between groups (e.g., Risk Difference = 0).
  
  * Alternative Hypothesis ($H_a$): There is a difference (e.g., Risk Difference $\neq$ 0).
  
  
### 2. Error Rates and Power

We deal with two types of errors because we are looking at a sample, not a census.

  * **Type 1 Error (alpha - $\alpha$ )** : The False Positive. Rejecting the null when it is actually true.
  
    * Standard: Usually set to 0.05 (5%).
    
  * **Type 2 Error (beta - $\beta$ )** :  The False Negative. Failing to reject the null when it is actually false (missing a real effect).
  
    * Standard: We usually aim for Power ($1 - \beta$) of 80%
    
    
If I decrease my Type I error rate (e.g., from 0.05 to 0.01) what happens to the required sample size?

  * It increases. Being more "strict" requires more evidence
    

### 3. Minimum Detectable Effect (MDE)

This is the smallest difference between groups that you want to be able to detect.

  * example: If the baseline illness rate is 3.4%, do you want to detect a jump to 3.5% (tiny effect) or 4.4% (larger effect)?
  
  * key concept: Smaller effects are harder to find. To detect a smaller MDE, you need a larger sample size
  
  
If I want to detect a smaller difference (smaller MDE), what happens to sample size?

  * It increases. It is harder to find a needle in a haystack than a crowbar.
  
  
### 4. Variability ( $\sigma$ or SD )

How much "noise" is in your data?

  * key concept: The more variable the outcome (higher Standard Deviation), the harder it is to see the signal. Higher variability requires a larger sample size
  
  
If my data is highly variable (high SD), what happens to sample size?

  * It increases. You need more data to see through the noise
  
  
  

## Visualizing Power (The "Two Curves" Concept)


visualize two bell curves: the Null Distribution (centered at 0 effect) and the Alternative Distribution (centered at the effect size you hope to find).


How do we increase Power (the area under the Alternative curve)?

  * **Increase the Effect Size:**  Move the two curves further apart. It is easier to tell them apart.
  
  * **Increase Sample Size:** This shrinks the Standard Error. Visually, this makes the bell curves "skinnier" and taller, so they overlap less.
  
  * **Increase Alpha:** Moving the critical value to the left increases power, but increases the risk of a False Positive.
  
  
  
## The Twist: Cluster Randomized Trials (CRTs)


This is a major focus of PHW250B. In many public health interventions (like the Bangladesh WASH study), we randomize villages or compounds, not individuals


  * **The Problem:** Independence In standard statistics, we assume every person is independent.In clusters, people within the same village are more similar to each other (correlated). This correlation provides less information than if they were independent

  * **The Solution:** The Design Effect (Deff) To calculate sample size for clusters, we calculate the sample size for individuals and then inflate it using the Design Effect
  
$$Deff = 1 + (m - 1)\rho$$ 

  * $m$: Average cluster size (number of people per cluster).
  
  * $\rho$ (Rho): Intraclass Correlation Coefficient (ICC). A measure of how correlated people are within a cluster
  
    * If $\rho = 0$: No correlation (same as individual RCT).
    
    * If $\rho = 1$: Everyone in the cluster is the same (Effective sample size = number of clusters)



#### Key Concept: 

Power in clustered designs is driven primarily by the number of clusters ($k$), NOT the number of people per cluster ($m$). Once you reach a certain cluster size (Rule of thumb: $m > 1/\rho$), adding more people to the cluster adds very little power


In a cluster trial, is it better to double the number of clusters or double the number of people per cluster?

  * Double the number of clusters. This adds more power




------------


## Practice Problems

For each of the following parameters: power, alpha, beta, and , state whether they
increase, decrease or stay the same for each of the following conditions

##### Increasing the sample size

Power: increases
Alpha: stays the same
Beta: decreases
Z1-ð›‚ : stays the same
Z1-ð›½: increases


##### Decreasing effect size of interest (e.g. relative risk of 1.5 instead of 3.0)

Power: decreases
Alpha: stays the same
Beta: increases
Z1-ð›‚ : stays the same
Z1-ð›½ : decreases



##### Using a two-sided test instead of a one-sided test

Power: decreases
Alpha: overall alpha stays the same, but decreases in each tail (technically correct
answer is that it stays the same)
Beta: increases
Z1-ð›‚ : Since the overall alpha stays the same, Z1-Î± also stays the same, though in the
actual calculation in this case we would be using Z1-Î±/2, which will be larger than Z1-Î±.
Z1-ð›½ : decreases




##### Decrease variability in the outcome measure

Power: increases
Alpha: stays the same
Beta: decreases
Z1-ð›‚ : stays the same
Z1-ð›½ : increases


-------------


You are interested in studying whether male college students who are members of a fraternity
drink more alcohol compared to male college students who are not fraternity members. 

You have conducted a survey of Berkeley students and ask them to report how many alcoholic
drinks they consumed in the past week. Among your sample of male students who are not
fraternity members, **the average number of drinks consumed per week is 5.** 

Your sample of fraternity members consume an average of 8 drinks per week. 

The standard error under the null and the alternative distributions for this two-sample difference in means is 1.5. 
(use the pnorm function in r to convert a Z score into a probability).

How much statistical power did you have to detect if fraternity members consume more alcohol per week compared to average Berkeley males? 

Assume a one-sided test and an alpha of 0.05. 

Note: This question must be solved by hand. See the Selvin reading page 84 â€“ you are calculating power for a difference in means, so your test statistic X is the difference in means.


ð»0: ðœ‡0 = 0

ð»a: ðœ‡ = 8 âˆ’ 5 = 3

SE0 = SE = 1.5

Z0.95 = 1.645


Power = 

P ( Z > (1.645 * 1.5 + (0-3)) / 1.5 ) == P(Z > -0.355)

Power = 0.6387052

```{r}
pnorm(0.355)
```


Interpretation: 

Given the sample size we have, the probability of detecting a difference between fraternity boys and non-fraternity members of 3 or more drinks per week was 0.64.


Type II error represents the probability of accepting the null hypothesis when the alternative hypothesis is true. 

In this case, the Type II error is 0.363 (or 0.36 if calculated as 1-power; the difference is due to the imprecision in the Z table).





